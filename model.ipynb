{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Final_Fashion_Items_Dataset_50k.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the descriptions\n",
    "encodings = tokenizer(data['Description'].tolist(), truncation=True, padding='max_length', max_length=512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    High-Waisted Skinny Jeans, Faux Leather Mini S...\n",
      "1    High-Waisted Skinny Jeans, Faux Leather Mini S...\n",
      "2    Knit Pullover Sweater, Floral Maxi Dress, Athl...\n",
      "3    Cropped Denim Jacket, Mesh Sports Leggings, Hi...\n",
      "4     Graphic Print T-shirt, High-Top Basketball Shoes\n",
      "Name: Recommended Matches, dtype: object\n",
      "Shape of labels array: (50000, 23)\n",
      "Label classes: ['Ankle Strap Heels' 'Athletic Track Jacket' 'Beaded Evening Clutch'\n",
      " 'Boho Style Fringed Vest' 'Canvas Sneakers' 'Casual Linen Blouse'\n",
      " 'Chunky Gold Hoop Earrings' 'Classic Trench Coat' 'Cropped Denim Jacket'\n",
      " 'Elegant Satin Blouse' 'Faux Leather Mini Skirt' 'Floral Maxi Dress'\n",
      " 'Graphic Print T-shirt' 'High-Top Basketball Shoes'\n",
      " 'High-Waisted Skinny Jeans' 'Knit Pullover Sweater'\n",
      " 'Leather Crossbody Bag' 'Mesh Sports Leggings'\n",
      " 'Pleated Chiffon Wide-Leg Pants' 'Silk Evening Gown'\n",
      " 'Sleeveless Skater Dress' 'Soft Cotton Scarf' 'Wool Fedora Hat']\n",
      "Labels tensor shape: torch.Size([50000, 23])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Final_Fashion_Items_Dataset_50k.csv')\n",
    "\n",
    "# Check the format of the 'Recommended Matches' column\n",
    "print(data['Recommended Matches'].head())\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "encodings = tokenizer(data['Description'].tolist(), max_length=128, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# MultiLabel Binarizer for encoding labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(data['Recommended Matches'].str.split(', '))\n",
    "\n",
    "# Verify the results\n",
    "print(f\"Shape of labels array: {labels.shape}\")\n",
    "print(f\"Label classes: {mlb.classes_}\")\n",
    "\n",
    "# Check for correct label tensor shape and classes\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "print(f\"Labels tensor shape: {labels_tensor.shape}\")\n",
    " \n",
    "combined_data = list(zip(encodings['input_ids'], encodings['attention_mask'], labels_tensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the combined data\n",
    "train_data, val_data = train_test_split(combined_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Unpack the training and validation data\n",
    "train_input_ids, train_attention_mask, train_labels = zip(*train_data)\n",
    "val_input_ids, val_attention_mask, val_labels = zip(*val_data)\n",
    "\n",
    "# Convert tuples back to tensors\n",
    "train_input_ids = torch.stack(train_input_ids)\n",
    "train_attention_mask = torch.stack(train_attention_mask)\n",
    "train_labels = torch.stack(train_labels)\n",
    "val_input_ids = torch.stack(val_input_ids)\n",
    "val_attention_mask = torch.stack(val_attention_mask)\n",
    "val_labels = torch.stack(val_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FashionDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "val_dataset = FashionDataset(val_input_ids, val_attention_mask, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(mlb.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15000 [08:25<?, ?it/s]\n",
      "  3%|▎         | 500/15000 [00:53<22:23, 10.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3043, 'grad_norm': 0.7466734647750854, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1000/15000 [01:38<19:53, 11.73it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2661, 'grad_norm': 0.6128305196762085, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1500/15000 [02:23<19:37, 11.46it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2636, 'grad_norm': 0.5651799440383911, 'learning_rate': 4.5e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2000/15000 [03:08<19:04, 11.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2633, 'grad_norm': 0.6109074354171753, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2500/15000 [03:54<18:11, 11.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2627, 'grad_norm': 0.5381370782852173, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3000/15000 [04:39<17:36, 11.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2618, 'grad_norm': 0.5983831286430359, 'learning_rate': 4e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 3500/15000 [05:27<17:10, 11.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2609, 'grad_norm': 0.5886273980140686, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4000/15000 [06:14<16:25, 11.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2597, 'grad_norm': 0.5998498797416687, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 4500/15000 [06:59<15:28, 11.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2592, 'grad_norm': 0.6379507780075073, 'learning_rate': 3.5e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5000/15000 [07:44<14:41, 11.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2606, 'grad_norm': 0.6207805871963501, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 33%|███▎      | 5001/15000 [08:52<33:37:50, 12.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2578980028629303, 'eval_runtime': 67.0385, 'eval_samples_per_second': 149.168, 'eval_steps_per_second': 18.646, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 5500/15000 [09:36<13:36, 11.64it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2596, 'grad_norm': 0.5176799297332764, 'learning_rate': 3.1666666666666666e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6000/15000 [10:21<13:02, 11.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2588, 'grad_norm': 0.5850674510002136, 'learning_rate': 3e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 6500/15000 [11:06<12:33, 11.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2597, 'grad_norm': 0.49838075041770935, 'learning_rate': 2.8333333333333335e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7000/15000 [11:51<11:40, 11.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2593, 'grad_norm': 0.49713367223739624, 'learning_rate': 2.6666666666666667e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 7500/15000 [12:36<11:10, 11.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2587, 'grad_norm': 0.4166128933429718, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8000/15000 [13:20<09:59, 11.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2586, 'grad_norm': 0.4045802354812622, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 8500/15000 [14:05<09:30, 11.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2584, 'grad_norm': 0.38991448283195496, 'learning_rate': 2.1666666666666667e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9000/15000 [14:50<08:36, 11.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2585, 'grad_norm': 0.4205186069011688, 'learning_rate': 2e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 9500/15000 [15:34<08:02, 11.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2577, 'grad_norm': 0.4141974151134491, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10000/15000 [16:20<07:30, 11.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2582, 'grad_norm': 0.4140099883079529, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 67%|██████▋   | 10001/15000 [16:35<3:44:23,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25672781467437744, 'eval_runtime': 13.5031, 'eval_samples_per_second': 740.569, 'eval_steps_per_second': 92.571, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 10500/15000 [17:20<06:48, 11.01it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2574, 'grad_norm': 0.5119720101356506, 'learning_rate': 1.5e-05, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11000/15000 [18:06<05:50, 11.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2578, 'grad_norm': 0.3671681880950928, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 11500/15000 [18:51<05:07, 11.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2595, 'grad_norm': 0.4591037333011627, 'learning_rate': 1.1666666666666668e-05, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12000/15000 [19:36<04:26, 11.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2571, 'grad_norm': 0.3896959722042084, 'learning_rate': 1e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 12500/15000 [20:21<03:41, 11.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2566, 'grad_norm': 0.3329217731952667, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13000/15000 [21:07<02:57, 11.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2582, 'grad_norm': 0.37055760622024536, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 13500/15000 [21:53<02:14, 11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2578, 'grad_norm': 0.4167674481868744, 'learning_rate': 5e-06, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14000/15000 [22:39<01:28, 11.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2567, 'grad_norm': 0.37434881925582886, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 14500/15000 [23:25<00:43, 11.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.258, 'grad_norm': 0.40314170718193054, 'learning_rate': 1.6666666666666667e-06, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [24:10<00:00, 11.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2568, 'grad_norm': 0.3662957549095154, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|██████████| 15000/15000 [24:24<00:00, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2561521828174591, 'eval_runtime': 13.124, 'eval_samples_per_second': 761.961, 'eval_steps_per_second': 95.245, 'epoch': 3.0}\n",
      "{'train_runtime': 1464.8859, 'train_samples_per_second': 81.918, 'train_steps_per_second': 10.24, 'train_loss': 0.2608523142496745, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15000, training_loss=0.2608523142496745, metrics={'train_runtime': 1464.8859, 'train_samples_per_second': 81.918, 'train_steps_per_second': 10.24, 'total_flos': 341645412240000.0, 'train_loss': 0.2608523142496745, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    evaluation_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:13<00:00, 91.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2561521828174591, 'eval_runtime': 13.6681, 'eval_samples_per_second': 731.63, 'eval_steps_per_second': 91.454, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "# Load the tokenizer and the model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model_path = './results/checkpoint-15000'  # Adjust if your model is saved elsewhere\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Final_Fashion_Items_Dataset_50k.csv')\n",
    "texts = data['Description'].tolist()  # Adjust the column name if different\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "batch_size = 32  # Adjust based on your memory constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "data = pd.read_csv('Final_Fashion_Items_Dataset_50k.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokenized_data = tokenizer(data['Description'].tolist(), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Move tensors to the same device as the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_ids = tokenized_data['input_ids'].to(device)\n",
    "attention_mask = tokenized_data['attention_mask'].to(device)\n",
    "\n",
    "# Load the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('./results/checkpoint-15000').to(device)\n",
    "model.eval()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "# Assuming you want the index with the highest probability\n",
    "predicted_indices = probabilities.argmax(dim=-1).cpu().numpy()\n",
    "\n",
    "# Translate predicted indices to your categories or labels\n",
    "# You need a mapping from indices to actual labels\n",
    "# Let's say this was your binarizer with classes learned during training\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(data['Recommended Matches'].str.split(', '))  # Assuming this is how you fit the binarizer\n",
    "\n",
    "# The classes_ attribute holds the order of your labels\n",
    "classes = mlb.classes_\n",
    "\n",
    "# Creating the index_to_label mapping\n",
    "index_to_label = {i: label for i, label in enumerate(classes)}\n",
    "\n",
    "predicted_labels = [index_to_label[idx] for idx in predicted_indices]\n",
    "\n",
    "# Add predictions to the original dataframe\n",
    "data['Predicted Category'] = predicted_labels\n",
    "\n",
    "# Save or process further as needed\n",
    "data.to_csv('predictions.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheinproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
